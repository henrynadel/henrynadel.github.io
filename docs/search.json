[
  {
    "objectID": "ethics_blog.html",
    "href": "ethics_blog.html",
    "title": "Project 4: Predictive Analytics at Target",
    "section": "",
    "text": "In the early 2010s, Target became widely known for using predictive analytics in ways that reshaped what retailers could infer from seemingly ordinary shopping patterns. As Charles Duhigg reported in The New York Times Magazine (2012), Target’s data science team analyzed years of transaction logs to detect subtle purchasing shifts—unscented lotion, certain supplements, cotton balls—that often correlated with early pregnancy. By combining these signals, the team built what they called a “pregnancy prediction” score and mailed coupons timed to a customer’s likely trimester (Duhigg, 2012).\nWhat stands out in this case is not simply the technical capability of the model, but the imbalance it created between the company and its customers. Most shoppers had no awareness that routine purchases could be interpreted as indicators of a private life event. The now-famous anecdote of the father who learned of his daughter’s pregnancy through Target’s mailed coupons illustrates this tension—predictive systems can surface intimate inferences long before individuals choose to share them. What Target described as personalization felt, to many, like an unexpected intrusion into family dynamics.\nJohn, Kim, and Barasz (2020) argue that cases like Target’s underscore the need for companies to “design for transparency,” suggesting that trust can be strengthened when consumers understand how their data is being used. Their point about transparency is valuable, but it may not fully resolve the discomfort raised by the Target example. Even if consumers knew more about the system, many would still object to a retailer using ordinary purchases to infer something as personal as pregnancy. In other words, the issue is not always misunderstanding—it is the nature of the inference itself.\n\n\n\n\n\n\nDuhigg (2012) explains that Target’s model was built by stitching together information from guest IDs, credit card transactions, coupons, website behavior, and store surveys. At no stage were customers explicitly asked whether this data could be used to generate predictions about pregnancy. Any “consent” technically existed only within broad loyalty-program disclaimers—documents that few people read, and that did not specify the possibility of intimate inferences being drawn from their behavior.\nJohn et al. (2020) argue that this falls short of meaningful consent, which they define as consent that involves clarity, purpose, and a sense of reciprocal benefit. The term “meaningful” here does not imply that Target violated its privacy policy—something we cannot determine from available reporting—but rather that customers had no reasonable way to anticipate how their data would be interpreted. The company may have met the minimal legal requirements of consent, but consumers lacked the information necessary to understand, let alone influence, how their purchasing behavior was being used.\nIt is also important to differentiate between inference and disclosure. While the mailed coupons did reveal something private, they did not make pregnancy status “fully public.” Instead, they introduced ambiguity and the possibility of unwanted inference within a household—something subtler than public disclosure but still ethically significant.\n\n\n\nTarget used a persistent “Guest ID” to link all aspects of a customer’s shopping life over time, effectively creating a long-term behavioral profile (Duhigg, 2012). Even if that ID was anonymized internally, the targeted coupons were sent to specific physical addresses, making the inference visible to anyone who handled the household mail. In practice, the model’s output moved beyond the database and into domestic space.\nJohn et al. (2020) suggest that transparency about data practices might improve consumer trust. But in this case, the discomfort does not stem from confusion about how data is used—it stems from the experience of being profiled in ways that feel intimate. Even a fully transparent system would still allow a company to infer something personal and then make that inference visible through marketing. The concern lies in the individualized nature of predictive profiling itself, not merely the lack of explanation around it.\n\n\n\nAccording to Duhigg (2012), the pregnancy model relied heavily on data from customers who had signed up for Target’s baby registry. This subset became the template for “pregnancy behavior,” despite representing only a fraction of expecting parents. People who paid in cash, shopped infrequently, or whose consumption patterns differed from registry users were more likely to be misclassified or overlooked entirely.\nJohn et al. (2020) describe this issue as a lack of “data empathy,” urging designers to consider people who fall outside the dominant patterns of a dataset. While that framing is helpful, it assumes that predictive analytics are inevitable and simply need to be fine-tuned. But in this case, the bias was not just a technical flaw—it reflected who Target considered worth identifying and targeting. The model made some pregnancies highly legible while others remained invisible, mirroring commercial priorities rather than offering a neutral representation of the population.\n\n\n\nTarget’s predictive system was not designed only to observe behavior—it aimed to shape it. Internally, marketers described pregnancy as a strategic moment for “habit formation,” when brand loyalty could be molded and strengthened (Duhigg, 2012). Predictive analytics here became a tool not just for anticipating future purchases but for subtly influencing them.\nJohn et al. (2020) imagine a future in which predictive analytics empower consumers and distribute agency more evenly. But this vision assumes a level of collaboration that is difficult to reconcile with existing retail incentives. When prediction is tied to profit, companies are rewarded for leveraging moments of vulnerability, not for sharing power with consumers. Target’s case shows how easily predictive capability becomes a mechanism of subtle influence rather than a shared resource.\n\n\n\n\n\nThe Target case demonstrates how predictive analytics, when deployed without meaningful consent or safeguards, can deepen existing power imbalances. The company gained a competitive advantage through early detection of pregnancy and improved customer retention. For consumers, the harms were less visible but still significant: a loss of control over how personal information—and the inferences drawn from it—circulated within their lives.\nJohn et al. (2020) encourage us to think more broadly about who gets to shape predictive systems and what values guide their design. That framing is helpful, but only if we acknowledge that transparency alone cannot resolve the deeper structural issues. A model can meet high technical standards and still struggle ethically if the people affected by it have little say in how it operates or what boundaries it observes.\nAcross other data-driven systems, similar tensions appear. Technologies built for efficiency often overlook the realities of the people they act upon. Target’s system shows how predictive analytics, when operating quietly and without clear limits, can cross lines that individuals never had the opportunity to draw themselves. It’s a reminder that ethical data practices require more than accuracy or optimization—they require attention to how these systems shape everyday life.\n\n\n\n\nDuhigg, C. (2012). How Companies Learn Your Secrets. The New York Times Magazine, February 19.\nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\nJohn, L. K., Kim, T., & Barasz, K. (2020). Customer Data: Designing for Transparency and Trust. Harvard Business Review, May–June.\nhttps://hbr.org/2020/05/customer-data-designing-for-transparency-and-trust"
  },
  {
    "objectID": "ethics_blog.html#scenario-overview",
    "href": "ethics_blog.html#scenario-overview",
    "title": "Project 4: Predictive Analytics at Target",
    "section": "",
    "text": "In the early 2010s, Target became widely known for using predictive analytics in ways that reshaped what retailers could infer from seemingly ordinary shopping patterns. As Charles Duhigg reported in The New York Times Magazine (2012), Target’s data science team analyzed years of transaction logs to detect subtle purchasing shifts—unscented lotion, certain supplements, cotton balls—that often correlated with early pregnancy. By combining these signals, the team built what they called a “pregnancy prediction” score and mailed coupons timed to a customer’s likely trimester (Duhigg, 2012).\nWhat stands out in this case is not simply the technical capability of the model, but the imbalance it created between the company and its customers. Most shoppers had no awareness that routine purchases could be interpreted as indicators of a private life event. The now-famous anecdote of the father who learned of his daughter’s pregnancy through Target’s mailed coupons illustrates this tension—predictive systems can surface intimate inferences long before individuals choose to share them. What Target described as personalization felt, to many, like an unexpected intrusion into family dynamics.\nJohn, Kim, and Barasz (2020) argue that cases like Target’s underscore the need for companies to “design for transparency,” suggesting that trust can be strengthened when consumers understand how their data is being used. Their point about transparency is valuable, but it may not fully resolve the discomfort raised by the Target example. Even if consumers knew more about the system, many would still object to a retailer using ordinary purchases to infer something as personal as pregnancy. In other words, the issue is not always misunderstanding—it is the nature of the inference itself."
  },
  {
    "objectID": "ethics_blog.html#ethical-analysis-four-issue-areas",
    "href": "ethics_blog.html#ethical-analysis-four-issue-areas",
    "title": "Project 4: Predictive Analytics at Target",
    "section": "",
    "text": "Duhigg (2012) explains that Target’s model was built by stitching together information from guest IDs, credit card transactions, coupons, website behavior, and store surveys. At no stage were customers explicitly asked whether this data could be used to generate predictions about pregnancy. Any “consent” technically existed only within broad loyalty-program disclaimers—documents that few people read, and that did not specify the possibility of intimate inferences being drawn from their behavior.\nJohn et al. (2020) argue that this falls short of meaningful consent, which they define as consent that involves clarity, purpose, and a sense of reciprocal benefit. The term “meaningful” here does not imply that Target violated its privacy policy—something we cannot determine from available reporting—but rather that customers had no reasonable way to anticipate how their data would be interpreted. The company may have met the minimal legal requirements of consent, but consumers lacked the information necessary to understand, let alone influence, how their purchasing behavior was being used.\nIt is also important to differentiate between inference and disclosure. While the mailed coupons did reveal something private, they did not make pregnancy status “fully public.” Instead, they introduced ambiguity and the possibility of unwanted inference within a household—something subtler than public disclosure but still ethically significant.\n\n\n\nTarget used a persistent “Guest ID” to link all aspects of a customer’s shopping life over time, effectively creating a long-term behavioral profile (Duhigg, 2012). Even if that ID was anonymized internally, the targeted coupons were sent to specific physical addresses, making the inference visible to anyone who handled the household mail. In practice, the model’s output moved beyond the database and into domestic space.\nJohn et al. (2020) suggest that transparency about data practices might improve consumer trust. But in this case, the discomfort does not stem from confusion about how data is used—it stems from the experience of being profiled in ways that feel intimate. Even a fully transparent system would still allow a company to infer something personal and then make that inference visible through marketing. The concern lies in the individualized nature of predictive profiling itself, not merely the lack of explanation around it.\n\n\n\nAccording to Duhigg (2012), the pregnancy model relied heavily on data from customers who had signed up for Target’s baby registry. This subset became the template for “pregnancy behavior,” despite representing only a fraction of expecting parents. People who paid in cash, shopped infrequently, or whose consumption patterns differed from registry users were more likely to be misclassified or overlooked entirely.\nJohn et al. (2020) describe this issue as a lack of “data empathy,” urging designers to consider people who fall outside the dominant patterns of a dataset. While that framing is helpful, it assumes that predictive analytics are inevitable and simply need to be fine-tuned. But in this case, the bias was not just a technical flaw—it reflected who Target considered worth identifying and targeting. The model made some pregnancies highly legible while others remained invisible, mirroring commercial priorities rather than offering a neutral representation of the population.\n\n\n\nTarget’s predictive system was not designed only to observe behavior—it aimed to shape it. Internally, marketers described pregnancy as a strategic moment for “habit formation,” when brand loyalty could be molded and strengthened (Duhigg, 2012). Predictive analytics here became a tool not just for anticipating future purchases but for subtly influencing them.\nJohn et al. (2020) imagine a future in which predictive analytics empower consumers and distribute agency more evenly. But this vision assumes a level of collaboration that is difficult to reconcile with existing retail incentives. When prediction is tied to profit, companies are rewarded for leveraging moments of vulnerability, not for sharing power with consumers. Target’s case shows how easily predictive capability becomes a mechanism of subtle influence rather than a shared resource."
  },
  {
    "objectID": "ethics_blog.html#why-this-matters-who-benefits-who-is-neglected-or-harmed",
    "href": "ethics_blog.html#why-this-matters-who-benefits-who-is-neglected-or-harmed",
    "title": "Project 4: Predictive Analytics at Target",
    "section": "",
    "text": "The Target case demonstrates how predictive analytics, when deployed without meaningful consent or safeguards, can deepen existing power imbalances. The company gained a competitive advantage through early detection of pregnancy and improved customer retention. For consumers, the harms were less visible but still significant: a loss of control over how personal information—and the inferences drawn from it—circulated within their lives.\nJohn et al. (2020) encourage us to think more broadly about who gets to shape predictive systems and what values guide their design. That framing is helpful, but only if we acknowledge that transparency alone cannot resolve the deeper structural issues. A model can meet high technical standards and still struggle ethically if the people affected by it have little say in how it operates or what boundaries it observes.\nAcross other data-driven systems, similar tensions appear. Technologies built for efficiency often overlook the realities of the people they act upon. Target’s system shows how predictive analytics, when operating quietly and without clear limits, can cross lines that individuals never had the opportunity to draw themselves. It’s a reminder that ethical data practices require more than accuracy or optimization—they require attention to how these systems shape everyday life."
  },
  {
    "objectID": "ethics_blog.html#citations",
    "href": "ethics_blog.html#citations",
    "title": "Project 4: Predictive Analytics at Target",
    "section": "",
    "text": "Duhigg, C. (2012). How Companies Learn Your Secrets. The New York Times Magazine, February 19.\nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\nJohn, L. K., Kim, T., & Barasz, K. (2020). Customer Data: Designing for Transparency and Trust. Harvard Business Review, May–June.\nhttps://hbr.org/2020/05/customer-data-designing-for-transparency-and-trust"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! My name is Henry Nadel. I am currently in my final year at Pitzer College where I study Environmental Analysis on a public policy track with an additional minor in Data Science. I am particularly interested in using data to inform public policy along with using graphing and GIS software to convey data in an accessible manner. When I’m not in the classroom I can usually be found outside running, climbing, or exploring one of California’s amazing parks.\nCurrently: Senior at Pitzer College\nMajor: Environmental Analysis; Minor: Data Science\nBased in: Claremont, CA\nOriginally from: Newton, MA"
  },
  {
    "objectID": "tidytuesday-1.html",
    "href": "tidytuesday-1.html",
    "title": "TidyTuesday - Himalayan Mountaineering",
    "section": "",
    "text": "The following data set was gathered and provided by TidyTuesday, linked here:\nhttps://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-01-21/readme.md\nThe data was sourced from the Himalayan Database which keeps information on peaks and mountaineering expeditions that take place in the Himalayan Region. The full Himalayan database catalog can be found here:\nhttps://www.himalayandatabase.com/index.html\nThe following is a basic data visualization looking to compare the volume of expeditions per region on all mountains exceeding 7,000 meters. Shown first is the R code used to produce the visualization, which joins the “peaks” and “expeditions” data sets, filters for only peaks above 7,000m, and then counts the expeditions by region. The final code chunk is a ggplot to visualize the data.\n\n# joining\nexped_tidy &lt;- exped_tidy |&gt; mutate(PEAKID = as.character(PEAKID))\npeaks_tidy &lt;- peaks_tidy |&gt; mutate(PEAKID = as.character(PEAKID))\n\n# count expeditions for peaks &gt;= 7000m \nexped_by_region &lt;- exped_tidy |&gt;\n  distinct(EXPID, PEAKID) |&gt;  \n  inner_join(\n    peaks_tidy |&gt; select(PEAKID, HEIGHTM, REGION_FACTOR),\n    by = \"PEAKID\"\n  ) |&gt;\n  filter(HEIGHTM &gt;= 7000) |&gt;\n  count(REGION_FACTOR, name = \"n_expeditions\") |&gt;\n  arrange(desc(n_expeditions))\n\nexped_by_region\n\n# A tibble: 6 × 2\n  REGION_FACTOR           n_expeditions\n  &lt;chr&gt;                           &lt;int&gt;\n1 Khumbu-Rolwaling-Makalu           356\n2 Manaslu-Ganesh                     96\n3 Annapurna-Damodar-Peri             90\n4 Dhaulagiri-Mukut                   45\n5 Kangchenjunga-Janak                28\n6 Langtang-Jugal                      2\n\n\n\n# ---- plot ----\nggplot(exped_by_region,\n       aes(x = forcats::fct_reorder(REGION_FACTOR, n_expeditions),\n           y = n_expeditions)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Number of Expeditions on ≥7000 m Peaks, by Region\",\n    subtitle = \"Himalayan Database (TidyTuesday 2025-01-21)\",\n    x = \"Region\",\n    y = \"Number of expeditions\"\n  ) +\n  theme_minimal(base_size = 12)"
  },
  {
    "objectID": "sql_project.html",
    "href": "sql_project.html",
    "title": "Analysing Policing with SQL",
    "section": "",
    "text": "In this project, I look at policing patterns across four New England states: Connecticut, Massachusetts, Rhode Island, and Vermont, using data from the Stanford Open Policing Project. Unfortunately, New Hampshire could not be included because the data did not include all of the necessary values. The goal of this research is simple: use SQL to pull together large statewide stop records, and then use R to visualize my two core questions:\n\nWhat is the racial breakdown of who is being searched during traffic stops?\nWhen searches do happen, at what rates is contraband found?\n\nThese questions were chosen because of the light they can shed on where officer discretion, racial bias, and assumptions about criminality become visible. By comparing search rates and hit rates side-by-side, we can see where enforcement patterns align with actual evidence, and where they don’t."
  },
  {
    "objectID": "sql_project.html#overview",
    "href": "sql_project.html#overview",
    "title": "Analysing Policing with SQL",
    "section": "",
    "text": "In this project, I look at policing patterns across four New England states: Connecticut, Massachusetts, Rhode Island, and Vermont, using data from the Stanford Open Policing Project. Unfortunately, New Hampshire could not be included because the data did not include all of the necessary values. The goal of this research is simple: use SQL to pull together large statewide stop records, and then use R to visualize my two core questions:\n\nWhat is the racial breakdown of who is being searched during traffic stops?\nWhen searches do happen, at what rates is contraband found?\n\nThese questions were chosen because of the light they can shed on where officer discretion, racial bias, and assumptions about criminality become visible. By comparing search rates and hit rates side-by-side, we can see where enforcement patterns align with actual evidence, and where they don’t."
  },
  {
    "objectID": "sql_project.html#search-rates-by-state-and-race",
    "href": "sql_project.html#search-rates-by-state-and-race",
    "title": "Analysing Policing with SQL",
    "section": "Search Rates by State and Race",
    "text": "Search Rates by State and Race\n\nSELECT\nstate,\nrace,\nCOUNT(*) AS n_stops,\nSUM(search_conducted) AS n_searches,\nROUND(100.0 * SUM(search_conducted) / COUNT(*), 1) AS search_rate_pct\nFROM (\nSELECT\n'CT' AS state,\nLOWER(subject_race) AS race,\nsearch_conducted,\ncontraband_found\nFROM ct_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n\nUNION ALL\nSELECT\n'MA',\nLOWER(subject_race),\nsearch_conducted,\ncontraband_found\nFROM ma_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n\nUNION ALL\nSELECT\n'RI',\nLOWER(subject_race),\nsearch_conducted,\ncontraband_found\nFROM ri_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n\nUNION ALL\nSELECT\n'VT',\nLOWER(subject_race),\nsearch_conducted,\ncontraband_found\nFROM vt_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n) AS ne_statewide\nGROUP BY state, race\nHAVING COUNT(*) &gt;= 100\nORDER BY state, race\n\n\nsearch_rates &lt;- search_rates |&gt;\n  filter(!is.na(race) & race != \"unknown\")\n\nsearch_rates$race &lt;- factor(search_rates$race)\nsearch_rates$state &lt;- factor(search_rates$state)\n\nggplot(search_rates,\naes(x = state, y = search_rate_pct, fill = race)) +\ngeom_col(position = \"dodge\") +\nscale_fill_brewer(palette = \"Set2\") +\nlabs(\ntitle = \"Search Rates by State and Race (CT, MA, RI, VT)\",\nx = \"State\",\ny = \"Search Rate (% of Stops)\",\nfill = \"Race\"\n) +\ntheme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nThis plot shows that search rates are not evenly distributed across racial groups, and the patterns differ across states. Across every state shown above, Black and Hispanic drivers are searched at substantially higher rates than white drivers. This is especially pronounced in Connecticut and Rhode Island, where Black drivers face search rates around 7%, compared to roughly 3% for white drivers. Hispanic drivers show similarly elevated rates—around 6–7% in CT and RI.\nMassachusetts and Vermont have lower overall search activity, but the racial pattern still remains: Black and Hispanic drivers are searched more than white drivers, while Asian/Pacific Islander drivers consistently face the lowest search rates across the region. The “other” and “unknown” categories vary but rarely approach the higher search pressure placed on Black and Hispanic motorists. What emerges here is not just cross-state variation but a consistent structure: race strongly predicts the likelihood of being searched, and white drivers are the least likely to be searched in every state in the dataset."
  },
  {
    "objectID": "sql_project.html#contraband-hit-rates-by-state-and-race-among-searched-stops",
    "href": "sql_project.html#contraband-hit-rates-by-state-and-race-among-searched-stops",
    "title": "Analysing Policing with SQL",
    "section": "Contraband Hit Rates by State and Race (Among Searched Stops)",
    "text": "Contraband Hit Rates by State and Race (Among Searched Stops)\n\nSELECT\nstate,\nrace,\nSUM(search_conducted) AS n_searches,\nSUM(CASE WHEN contraband_found = 1 THEN 1 ELSE 0 END) AS n_hits,\nROUND(\n100.0 *\nSUM(CASE WHEN contraband_found = 1 THEN 1 ELSE 0 END)\n/ NULLIF(SUM(search_conducted), 0),\n1\n) AS hit_rate_pct\nFROM (\nSELECT\n'CT' AS state,\nLOWER(subject_race) AS race,\nsearch_conducted,\ncontraband_found\nFROM ct_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n\nUNION ALL\nSELECT\n'MA',\nLOWER(subject_race),\nsearch_conducted,\ncontraband_found\nFROM ma_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n\nUNION ALL\nSELECT\n'RI',\nLOWER(subject_race),\nsearch_conducted,\ncontraband_found\nFROM ri_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n\nUNION ALL\nSELECT\n'VT',\nLOWER(subject_race),\nsearch_conducted,\ncontraband_found\nFROM vt_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n) AS ne_statewide\nGROUP BY state, race\nHAVING SUM(search_conducted) &gt;= 50\nORDER BY state, race\n\n\nhit_rates &lt;- hit_rates |&gt;\n  filter(!is.na(race) & race != \"unknown\")\n\nhit_rates$race &lt;- factor(hit_rates$race)\n\nggplot(hit_rates,\naes(x = race, y = hit_rate_pct, fill = state)) +\ngeom_col(position = \"dodge\") +\nscale_fill_brewer(palette = \"Set2\") +\nlabs(\ntitle = \"Contraband Hit Rates by Race and State \\n (Among Searched Stops)\",\nx = \"Race\",\ny = \"Hit Rate (% of Searches)\"\n) +\ntheme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nThe second plot flips the lens: instead of who gets searched, it explores what police actually find when they conduct a search. The results tell a different story, one that goes against an assumption that higher search rates correspond to higher levels of contraband.\nAcross states, white drivers consistently show some of the highest contraband hit rates, especially in Vermont, where searches of white drivers turn up contraband nearly 90% of the time. Black and Hispanic drivers, who are searched far more often, show much lower hit rates—usually ranging from the high teens to the mid-40s depending on the state. This mismatch—higher search rates paired with lower hit rates—is especially stark in Connecticut and Rhode Island.\nIn short, the communities searched most aggressively are not the ones most likely to be found with contraband. Instead, white drivers, searched the least often, show the highest probability of a “successful” search.\nThis pattern displays a clear racial disparity in search rates, and shows that they are driven more by bias and over-policing, rather than by actual behavior."
  },
  {
    "objectID": "sql_project.html#conclusion",
    "href": "sql_project.html#conclusion",
    "title": "Analysing Policing with SQL",
    "section": "Conclusion",
    "text": "Conclusion\nBy merging four statewide datasets and visualizing search and hit rates, a clear pattern emerges: Black and Hispanic drivers are searched far more often, yet those searches uncover contraband less frequently than searches of white drivers. This holds across every state in the dataset. In other words, the groups searched the most are not the ones most likely to have contraband. That mismatch points toward discretionary enforcement shaped more by racial bias than by evidence. Even with New Hampshire excluded, the trend is unmistakable: search practices in these New England states fall unevenly along racial lines, and the data make that disparity impossible to explain away on the basis of “risk.\n\nDBI::dbDisconnect(con_traffic)"
  },
  {
    "objectID": "sql_project.html#citation",
    "href": "sql_project.html#citation",
    "title": "Analysing Policing with SQL",
    "section": "Citation:",
    "text": "Citation:\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "tidytuesday-2.html",
    "href": "tidytuesday-2.html",
    "title": "TidyTuesday - Gas Pricing",
    "section": "",
    "text": "The following data was provided by TidyTuesday, linked here:\nhttps://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-07-01/readme.md\nThe data was sourced from the U.S. Energy Information Administration (EIA), which publishes average retail gasoline and diesel prices each Monday. The original data was released by the U.S. Energy Information Administration, linked here: eia.gov/petroleum/gasdiesel\nThe following code averages out weekly reports to find average annual gasoline prices in the United States. Then, the data is graphed to create a visual display of the change in American gas pricing over time.\n\n# yearly average gasoline price\ngas_yearly &lt;- weekly_gas_prices |&gt;\n  filter(fuel == \"gasoline\", grade == \"all\", formulation == \"all\") |&gt;\n  mutate(year = year(date)) |&gt;\n  group_by(year) |&gt;\n  summarise(avg_price = mean(price, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(year)\n\nThe following is a printed list of the average price of gas in the U.S. from 1993 to 2025.\n\n\n# A tibble: 33 × 2\n    year avg_price\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1  1993      1.07\n 2  1994      1.08\n 3  1995      1.16\n 4  1996      1.24\n 5  1997      1.24\n 6  1998      1.07\n 7  1999      1.18\n 8  2000      1.52\n 9  2001      1.46\n10  2002      1.39\n11  2003      1.60\n12  2004      1.89\n13  2005      2.31\n14  2006      2.62\n15  2007      2.84\n16  2008      3.30\n17  2009      2.41\n18  2010      2.84\n19  2011      3.58\n20  2012      3.68\n21  2013      3.58\n22  2014      3.44\n23  2015      2.52\n24  2016      2.25\n25  2017      2.53\n26  2018      2.81\n27  2019      2.69\n28  2020      2.26\n29  2021      3.10\n30  2022      4.06\n31  2023      3.63\n32  2024      3.42\n33  2025      3.25\n\n\n\n# Plot\nggplot(gas_yearly, aes(x = year, y = avg_price, group = 1)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Average U.S. Gasoline Price by Year\",\n    x = \"Year\",\n    y = \"Average price (USD)\"\n  ) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  theme_minimal(base_size = 12)"
  },
  {
    "objectID": "Permutation_test.html",
    "href": "Permutation_test.html",
    "title": "Permutation Testing",
    "section": "",
    "text": "In this project, I conduct a small simulation study using a permutation test to explore whether there is an association between warehouse construction and unemployment rates in Southern California between 1990 and 2023.\nSpecifically, I ask:\nIs there a relationship between economic growth through warehouse construction and local unemployment levels in Los Angeles, San Bernardino, and Riverside Counties from 1990 to 2023? I ask this question because warehouse development is often seen as an engine of local job creation in the Inland Empire, but data can reveal whether such development truly corresponds with lower unemployment."
  },
  {
    "objectID": "Permutation_test.html#introduction",
    "href": "Permutation_test.html#introduction",
    "title": "Permutation Testing",
    "section": "",
    "text": "In this project, I conduct a small simulation study using a permutation test to explore whether there is an association between warehouse construction and unemployment rates in Southern California between 1990 and 2023.\nSpecifically, I ask:\nIs there a relationship between economic growth through warehouse construction and local unemployment levels in Los Angeles, San Bernardino, and Riverside Counties from 1990 to 2023? I ask this question because warehouse development is often seen as an engine of local job creation in the Inland Empire, but data can reveal whether such development truly corresponds with lower unemployment."
  },
  {
    "objectID": "Permutation_test.html#data-used",
    "href": "Permutation_test.html#data-used",
    "title": "Permutation Testing",
    "section": "Data Used:",
    "text": "Data Used:\nWarehouseCITY dataset1 — total new warehouse square footage built per year (1990–2023).\nLocal Area Unemployment Statistics (LAUS) from the U.S. Bureau of Labor Statistics2 — annual unemployment rate for the same region.\nBoth of the above data sets were sourced and trimmed down to only the fields relevant to this question during a an old project I had been working on.\n\n\nCode\nnew_sqft_by_unemployment &lt;- tribble(\n  ~Year, ~Building_sqft, ~Unemployment_rate,\n  1990, 33117000, 5.5,\n  1991, 18049000, 7.7,\n  1992, 13342000, 9.4,\n  1993, 5496000, 9.6,\n  1994, 9896000, 8.6,\n  1995, 12322000, 7.6,\n  1996, 13340000, 7.4,\n  1997, 18313000, 6.2,\n  1998, 40683000, 5.8,\n  1999, 37391000, 5.1,\n  2000, 43402000, 4.9,\n  2001, 43587000, 5.3,\n  2002, 29485000, 6.4,\n  2003, 32292000, 6.5,\n  2004, 36145000, 5.9,\n  2005, 37454000, 5.0,\n  2006, 36564000, 4.5,\n  2007, 36654000, 5.0,\n  2008, 22418000, 7.3,\n  2009, 12283000, 11.6,\n  2010, 4915000, 12.4,\n  2011, 8212000, 11.9,\n  2012, 13895000, 10.6,\n  2013, 18823000, 9.2,\n  2014, 23496000, 7.7,\n  2015, 29009000, 6.3,\n  2016, 28887000, 5.2,\n  2017, 37838000, 4.7,\n  2018, 45736000, 4.2,\n  2019, 30398000, 4.1,\n  2020, 37704000, 11.1,\n  2021, 43896000, 8.0,\n  2022, 44088000, 4.5,\n  2023, 30926000, 4.7\n)"
  },
  {
    "objectID": "Permutation_test.html#visualizing-the-original-relationship",
    "href": "Permutation_test.html#visualizing-the-original-relationship",
    "title": "Permutation Testing",
    "section": "Visualizing the Original Relationship",
    "text": "Visualizing the Original Relationship\nThe following scatterplot displays the annual relationship between new warehouse square footage and unemployment rate. Each point represents one year between 1990 and 2023, with a linear trend line to visualize direction.\n\n\nCode\nnew_sqft_by_unemployment |&gt;\n  ggplot(aes(x = Unemployment_rate, y = Building_sqft)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Building Square Footage vs. Unemployment Rate\",\n       x = \"Unemployment rate (%)\",\n       y = \"New building sq.ft. by year\") +\n  scale_x_continuous(labels = comma_format(accuracy = 1)) +\n  theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIn this graph we see years with higher construction volumes tend to have lower unemployment, suggesting a potential negative correlation (although not an extremely strong one). However, to test whether this observed relationship could have arisen by random chance, we perform a permutation test."
  },
  {
    "objectID": "Permutation_test.html#setting-up-the-permutation-test",
    "href": "Permutation_test.html#setting-up-the-permutation-test",
    "title": "Permutation Testing",
    "section": "Setting Up the Permutation Test",
    "text": "Setting Up the Permutation Test\nMy hypotheses are:\n  Null hypothesis (H₀): There is no association between warehouse construction and unemployment rate.\n  Alternative hypothesis (H₁): Higher levels of warehouse construction correspond with lower unemployment.\nThe test statistic will be the correlation coefficient between Building_sqft and Unemployment_rate."
  },
  {
    "objectID": "Permutation_test.html#simulation-design",
    "href": "Permutation_test.html#simulation-design",
    "title": "Permutation Testing",
    "section": "Simulation Design",
    "text": "Simulation Design\nI’ll first write a small function that computes a correlation after shuffling one variable. Then I’ll use a map() variant to repeat this process thousands of times, generating the null distribution.\n\n\nCode\n# Function to compute one permuted correlation\n# perm_cor for permutation correlation\nperm_cor &lt;- function(df) {\ncor(df$Building_sqft, sample(df$Unemployment_rate))\n}\n\n# Run simulations using map_dbl()\n\nset.seed(47)\nn_sims &lt;- 10000\nr_perm &lt;- map_dbl(1:n_sims, ~ perm_cor(new_sqft_by_unemployment))\n\n# Observed correlation in original data\nr_obs &lt;- cor(new_sqft_by_unemployment$Building_sqft,\nnew_sqft_by_unemployment$Unemployment_rate)\n\n# Two-sided p-value\np_val &lt;- mean(abs(r_perm) &gt;= abs(r_obs))\n\ntibble(observed_r = r_obs, p_value_two_sided = p_val)\n\n\n# A tibble: 1 × 2\n  observed_r p_value_two_sided\n       &lt;dbl&gt;             &lt;dbl&gt;\n1     -0.736                 0"
  },
  {
    "objectID": "Permutation_test.html#visualizing-the-permutation-distribution",
    "href": "Permutation_test.html#visualizing-the-permutation-distribution",
    "title": "Permutation Testing",
    "section": "Visualizing the Permutation Distribution",
    "text": "Visualizing the Permutation Distribution\nThe histogram below shows the distribution of correlation coefficients obtained under the null hypothesis (no relationship). The vertical red line marks the observed correlation from the real data.\n\n\nCode\ntibble(r = r_perm) |&gt;\nggplot(aes(x = r)) +\ngeom_histogram(bins = 40, fill = \"skyblue\", color = \"white\") +\ngeom_vline(xintercept = r_obs, color = \"red\", linewidth = 1.2) +\nlabs(\ntitle = \"Permutation Test for Correlation\",\nsubtitle = paste0(\"Observed r = \", round(r_obs, 3),\n\" | p(two-sided) = \", signif(p_val, 3)),\nx = \"Permuted correlation (null distribution)\",\ny = \"Count\"\n) +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nThe histogram is centered around 0, as expected under the null hypothesis. The observed correlation of approximately −0.74 lies far in the left tail of the distribution. Out of 10,000 permutations, none produced a correlation as extreme, implying a very small p-value and evidence against the null hypothesis."
  },
  {
    "objectID": "Permutation_test.html#discussion",
    "href": "Permutation_test.html#discussion",
    "title": "Permutation Testing",
    "section": "Discussion",
    "text": "Discussion\nThis simulation suggests a negative relationship between warehouse construction and unemployment in the Los Angeles–San Bernardino–Riverside region from 1990 to 2023, meaning that in years when more warehouse square footage was built, unemployment rates tended to be lower. However, it’s important to note that while the null hypothesis was disproved, that doesn’t mean the initial data shown at the top has a strong linear correlation (which is why this data got scrapped from the project i was originally using it for)\nFurthermore, this result does not imply direct causation. Both variables may be influenced by larger economic cycles (e.g., recessions in 2008 and 2020). Still, this permutation study demonstrated a clear relationship in the data, that with the help of permutation testing we can now attribute to more than just chance."
  },
  {
    "objectID": "Permutation_test.html#footnotes",
    "href": "Permutation_test.html#footnotes",
    "title": "Permutation Testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPhillips, S. A., & McCarthy, M. C. (2024). Warehouse CITY: An open data product for evaluating warehouse land-use in Southern California. Environment and Planning B, 51(8), 1965–1973. https://doi.org/10.1177/23998083241262553↩︎\nU.S. Bureau of Labor Statistics. (2024). Local Area Unemployment Statistics (LAUS). https://www.bls.gov/lau/↩︎"
  },
  {
    "objectID": "text_analysis.html",
    "href": "text_analysis.html",
    "title": "Analysing New York Times Headlines",
    "section": "",
    "text": "On this page I analyze a dataset of headlines from all New York Times articles dating back to 1996. My goal is to explore patterns in three areas:\nThis dataset comes from the NYTimes sample data included in the RTextTools package, which provides a structured and labeled collection of headline text suitable for text-mining exercises."
  },
  {
    "objectID": "text_analysis.html#frequest-word-usage",
    "href": "text_analysis.html#frequest-word-usage",
    "title": "Analysing New York Times Headlines",
    "section": "Frequest Word Usage",
    "text": "Frequest Word Usage\nThe following code cleans the headline text by lowercasing, removing punctuation, and stripping extra spaces. It then tokenizes the titles into individual words, removes stop words (like “the,” “and,” etc.), filters out very short strings, and counts the most frequent content words. The final object, top_words, keeps only the 15 most common meaningful words.\n\ntitles_tokens &lt;- nyt |&gt;\n  transmute(\n    Title_clean = Title |&gt;\n      str_to_lower() |&gt;\n      str_replace_all(\"[^a-z\\\\s]\", \" \") |&gt;\n      str_replace_all(\"\\\\s+\", \" \") |&gt;\n      str_trim()\n  ) |&gt;\n  unnest_tokens(word, Title_clean, token = \"words\") |&gt;\n  anti_join(stop_words, by = \"word\") |&gt;\n  filter(str_detect(word, \"^[a-z]+$\"), nchar(word) &gt;= 3)\n\ntop_words &lt;- titles_tokens |&gt;\n  count(word, sort = TRUE) |&gt;\n  slice_head(n = 15)\n\n\n# Plot — Top content words\nggplot(top_words, aes(x = reorder(word, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Most Frequent\n    VWords in NYT Headlines since 1996\",\n    x = \"Word\",\n    y = \"Count\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nThis bar chart shows the most common words in found in the headline column of the data set. It’s an interesting way to find many of the most common topics covered by the New York Times over the past three decades. Notably, the top three words stand distinctively above the rest in usage: Bush, Iraq, and War. this makes sense as all three of these words point to the same general event of President Bush’s decision to invade Iraq following 9/11. This makes sense as it was a large global event that took place near the beginning of this data’s period of observation. These findings make an interesting claim about how extensively the war in Iraq penetrated the American news cycle."
  },
  {
    "objectID": "text_analysis.html#frequently-mentioned-places",
    "href": "text_analysis.html#frequently-mentioned-places",
    "title": "Analysing New York Times Headlines",
    "section": "Frequently Mentioned Places",
    "text": "Frequently Mentioned Places\nin this section I find the locations that are most frequestly refrenced by searcing for words that follow the preposition “in…”\nThe code extracts any capitalized phrase following the word “in,” cleans the extracted place strings, counts how often each appears, and returns the top ten most frequent location phrases.\n\nlocs &lt;- nyt |&gt;\n  transmute(\n    Title,\n    loc = str_extract_all(\n      Title,\n      \"(?&lt;=\\\\bin\\\\s)([A-Z][A-Za-z\\\\-]+(?:\\\\s[A-Z][A-Za-z\\\\-]+)*)\"\n    )\n  ) |&gt;\n  unnest(loc) |&gt;\n  mutate(loc = loc |&gt;\n           str_replace_all(\"\\\\s+\", \" \") |&gt;\n           str_trim()) |&gt;\n  count(loc, sort = TRUE) |&gt;\n  slice_head(n = 10)\n\n\n# Plot 2 — Top phrases following \"in \"\nggplot(locs, aes(x = reorder(loc, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Phrases Following 'in' - NYT Headlines\",\n    x = \"Location phrase\",\n    y = \"Count\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nThe clear standout here is Iraq, which again reinforces how central the region was to U.S. news coverage in the early 2000s. Interestingly, the list is dominated by Middle Eastern and European locations, while continents such as Africa and South America do not appear in the top ten at all, revealing something about the NYT’s geographic priorities and what kinds of crises receive sustained attention. Among U.S. locations, only New York and Florida make the list, pointing to the newspaper’s home base and what I would guess is the frequency of election-related stories tied to Florida. These counts measure the frequency of locations framed as settings (“in ___”), not total mentions, making the pattern a bit more precise in identifying where events are depicted as happening.\nIt is worth noting that many of these names have much higher tallies when counting total uses. The specific criteria of words following “in” is meant to isolate the places where stories are taking place opposed to total mentions, however, this does introduce a limitation to the findings of this table."
  },
  {
    "objectID": "text_analysis.html#headlines-containing-numbers-years",
    "href": "text_analysis.html#headlines-containing-numbers-years",
    "title": "Analysing New York Times Headlines",
    "section": "Headlines Containing Numbers & Years",
    "text": "Headlines Containing Numbers & Years\nNumbers appear in headlines for many reasons: summaries of statistics, references to specific quantities, and, importantly, mentions of historical and future years. This section measures how common numeric content is and which years are referenced most often.\nThe following code checks whether each headline contains any number and separately extracts four-digit years beginning with 19xx or 20xx. It then calculates the share of headlines containing numbers.\n\ntitle_nums &lt;- nyt |&gt;\n  transmute(\n    Title,\n    has_number = str_detect(Title, \"\\\\b\\\\d{1,3}(?:,\\\\d{3})*(?:\\\\.\\\\d+)?\\\\b\"),\n    years_list = str_extract_all(Title, \"\\\\b(19|20)\\\\d{2}\\\\b\")\n  )\n\nprop_table &lt;- tibble(\n  metric = c(\"Headlines containing any number\"),\n  value  = c(mean(title_nums$has_number))\n) |&gt;\n  mutate(share = scales::percent(value)) |&gt;\n  select(metric, share)\n\n\n# Table — Share of headlines with numbers / percentages\nkable(\n  prop_table,\n  col.names = c(\"Metric\", \"Share of headlines\"),\n  caption = \"% of NYT headlines with numeric content\"\n)\n\n\n% of NYT headlines with numeric content\n\n\nMetric\nShare of headlines\n\n\n\n\nHeadlines containing any number\n11%\n\n\n\n\n\nRoughly 11% of headlines include at least one number—meaning that while numerical information is not dominant in headline writing, it still appears with some regularity"
  }
]