[
  {
    "objectID": "ethics_blog.html",
    "href": "ethics_blog.html",
    "title": "Project 4: Predictive Analytics at Target",
    "section": "",
    "text": "In the early 2010s, Target became widely known for using predictive analytics in ways that reshaped what retailers could infer from ordinary shopping patterns. As Charles Duhigg reported in The New York Times Magazine (2012), Target’s data science team analyzed years of transaction logs to pick up on subtle shifts—unscented lotion, certain supplements, cotton balls—that often signaled early pregnancy. By assembling these signals, they built what they called a “pregnancy prediction” score and deployed targeted coupons timed to match a customer’s likely trimester.\nWhat stood out in this case was not the model itself, but the sharp imbalance it created between the company and its customers. Most people shopping at Target had no sense that routine purchases were being interpreted as clues about a private stage of life. The anecdote of the father who learned of his daughter’s pregnancy through Target’s coupons captures this tension clearly: predictive systems can move faster and more invasively than social norms around privacy or disclosure. What the company described as personalization felt, for many, like surveillance.\nWriters in Harvard Business Review (HBR) John, Kim, and Barasz (2020), later suggested that cases like Target’s show why companies should “design for transparency.” Their argument rests on the idea that predictive analytics are inevitable, and that trust can be repaired if consumers are more involved. This view makes sense at a high level, but it also sidesteps a deeper issue: transparency may not resolve the discomfort people feel when companies use data to anticipate intimate parts of their lives. Sometimes the problem is not that consumers don’t understand the system—it’s that the system is doing something they would never knowingly agree to.\n\n\n\n\n\n\nAs Duhigg describes, Target built its pregnancy model by stitching together an assortment of everyday data sources: guest IDs, credit card transactions, coupon use, website behavior, and store surveys. At no point were customers explicitly asked whether this information could be used to infer pregnancy. Consent was hidden inside broad loyalty-program disclaimers that most people never read.\nThe HBR authors argue that this is a failure of meaningful consent. They emphasize that real consent requires clarity, purpose, and some sense of value returned to the consumer. In this case, however, the benefits flowed almost entirely to Target. Customers were not just unaware, they were structurally excluded from knowing how their data were being interpreted.\n\n\n\nEach customer’s behavior was tracked through a persistent “Guest ID,” creating a long-term map of their purchasing life. Even if names were removed internally, the targeted coupons mailed to specific households made the inference effectively public. A prediction—correct or not—arrived in the mailbox as a tangible object, visible to anyone in the home.\nThe HBR perspective emphasizes transparency, suggesting that if people better understood how their data were used, they would feel more comfortable. I disagree. The discomfort here is not rooted in confusion; it is rooted in the fact that predictive profiling transforms private behaviors into labels that can spill into the physical world. Transparency does nothing to change the basic dynamic in which a company can infer something personal and then disclose it, intentionally or not, through marketing. The issue is inherent to individualized profiling itself, not merely the lack of explanation around it.\n\n\n\nTarget’s model relied heavily on data from customers who had signed up for Targets baby registry, a group that reflects only a portion of expecting parents. Their behaviors became the template for the algorithm, meaning the system encoded norms from a small sample about what pregnancy “looks like” in Target’s data. People who pay in cash, shop infrequently, or whose consumption patterns diverge from registry users could be missed or misclassified.\nHBR refers to this as a lack of “data empathy,” a concept encouraging designers to consider how systems affect people who don’t match the dataset’s dominant patterns. While this is a valuable lens, it still accepts the premise that large-scale consumer prediction is a neutral or inevitable practice that simply needs fine-tuning. But Target’s model did more than misrepresent some consumers—it helped define whose lives were legible to the system and whose were not. Weather it was inadvertent or not, this bias is a reflection of who Target considered worth identifying and targeting in the first place.\n\n\n\nTarget’s system was not a passive observer. It used pregnancy predictions to shape long-term shopping habits, described internally as “habit formation.” This strategy turns Pregnancy, which can be understood as a period of heightened vulnerability for a consumer, into a commercial entry point. This is an example of predictive analytics doing more than just anticipating behavior, instead, through the process of habit building it sought to mold it.\nHBR imagines a different future in which predictive analytics empower consumers and distribute agency more evenly. But this vision relies on a kind of collaboration that is difficult to imagine within existing retail incentives. Target’s approach shows how easily predictive power becomes a tool for subtle influence. When prediction is tied to increased profit, corporations loose all incentive to collaborate or empathize with the concerns of consumers.\n\n\n\n\n\nThe Target case demonstrates how predictive analytics, when deployed without meaningful consent or accountability, can reinforce existing power imbalances. The company gained a competitive advantage through early detection of pregnancy and stronger customer retention. For consumers, however, the costs were harder to see but deeply felt: a loss of control over how their personal information and the inferences built from it circulated through their lives.\nHBR encourages us to view this as part of a broader question about who gets to shape the landscape of predictive technology and what the implications of there choices are. That framing is useful, but only if we recognize that transparency alone cannot resolve the underlying asymmetry. A technically sophisticated model can still fail sociall or ethicall standards, because people do not experience systems as abstractions, their experiences are formed by the ways those systems touch their everyday lives.\nLooking across other data-driven systems, the pattern is familiar. Technologies built to optimize efficiency often overlook the realities of those they are applied to. It’s an important reminder of the ways that code can touch peoples lives, and that efficiency (especially when geared towards profit) is far from the only metric to needed to create ethical processing systems. In the case of Target, we see how when predictive systems operate quietly, without clear boundaries, they risk crossing lines that individuals never had the chance to draw themselves.\n\n\n\n\nDuhigg, C. (2012). How Companies Learn Your Secrets. The New York Times Magazine, February 19.\nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\nJohn, L. K., Kim, T., & Barasz, K. (2020). Customer Data: Designing for Transparency and Trust. Harvard Business Review, May–June.\nhttps://hbr.org/2020/05/customer-data-designing-for-transparency-and-trust"
  },
  {
    "objectID": "ethics_blog.html#scenario-overview",
    "href": "ethics_blog.html#scenario-overview",
    "title": "Project 4: Predictive Analytics at Target",
    "section": "",
    "text": "In the early 2010s, Target became widely known for using predictive analytics in ways that reshaped what retailers could infer from ordinary shopping patterns. As Charles Duhigg reported in The New York Times Magazine (2012), Target’s data science team analyzed years of transaction logs to pick up on subtle shifts—unscented lotion, certain supplements, cotton balls—that often signaled early pregnancy. By assembling these signals, they built what they called a “pregnancy prediction” score and deployed targeted coupons timed to match a customer’s likely trimester.\nWhat stood out in this case was not the model itself, but the sharp imbalance it created between the company and its customers. Most people shopping at Target had no sense that routine purchases were being interpreted as clues about a private stage of life. The anecdote of the father who learned of his daughter’s pregnancy through Target’s coupons captures this tension clearly: predictive systems can move faster and more invasively than social norms around privacy or disclosure. What the company described as personalization felt, for many, like surveillance.\nWriters in Harvard Business Review (HBR) John, Kim, and Barasz (2020), later suggested that cases like Target’s show why companies should “design for transparency.” Their argument rests on the idea that predictive analytics are inevitable, and that trust can be repaired if consumers are more involved. This view makes sense at a high level, but it also sidesteps a deeper issue: transparency may not resolve the discomfort people feel when companies use data to anticipate intimate parts of their lives. Sometimes the problem is not that consumers don’t understand the system—it’s that the system is doing something they would never knowingly agree to."
  },
  {
    "objectID": "ethics_blog.html#ethical-analysis-four-issue-areas",
    "href": "ethics_blog.html#ethical-analysis-four-issue-areas",
    "title": "Project 4: Predictive Analytics at Target",
    "section": "",
    "text": "As Duhigg describes, Target built its pregnancy model by stitching together an assortment of everyday data sources: guest IDs, credit card transactions, coupon use, website behavior, and store surveys. At no point were customers explicitly asked whether this information could be used to infer pregnancy. Consent was hidden inside broad loyalty-program disclaimers that most people never read.\nThe HBR authors argue that this is a failure of meaningful consent. They emphasize that real consent requires clarity, purpose, and some sense of value returned to the consumer. In this case, however, the benefits flowed almost entirely to Target. Customers were not just unaware, they were structurally excluded from knowing how their data were being interpreted.\n\n\n\nEach customer’s behavior was tracked through a persistent “Guest ID,” creating a long-term map of their purchasing life. Even if names were removed internally, the targeted coupons mailed to specific households made the inference effectively public. A prediction—correct or not—arrived in the mailbox as a tangible object, visible to anyone in the home.\nThe HBR perspective emphasizes transparency, suggesting that if people better understood how their data were used, they would feel more comfortable. I disagree. The discomfort here is not rooted in confusion; it is rooted in the fact that predictive profiling transforms private behaviors into labels that can spill into the physical world. Transparency does nothing to change the basic dynamic in which a company can infer something personal and then disclose it, intentionally or not, through marketing. The issue is inherent to individualized profiling itself, not merely the lack of explanation around it.\n\n\n\nTarget’s model relied heavily on data from customers who had signed up for Targets baby registry, a group that reflects only a portion of expecting parents. Their behaviors became the template for the algorithm, meaning the system encoded norms from a small sample about what pregnancy “looks like” in Target’s data. People who pay in cash, shop infrequently, or whose consumption patterns diverge from registry users could be missed or misclassified.\nHBR refers to this as a lack of “data empathy,” a concept encouraging designers to consider how systems affect people who don’t match the dataset’s dominant patterns. While this is a valuable lens, it still accepts the premise that large-scale consumer prediction is a neutral or inevitable practice that simply needs fine-tuning. But Target’s model did more than misrepresent some consumers—it helped define whose lives were legible to the system and whose were not. Weather it was inadvertent or not, this bias is a reflection of who Target considered worth identifying and targeting in the first place.\n\n\n\nTarget’s system was not a passive observer. It used pregnancy predictions to shape long-term shopping habits, described internally as “habit formation.” This strategy turns Pregnancy, which can be understood as a period of heightened vulnerability for a consumer, into a commercial entry point. This is an example of predictive analytics doing more than just anticipating behavior, instead, through the process of habit building it sought to mold it.\nHBR imagines a different future in which predictive analytics empower consumers and distribute agency more evenly. But this vision relies on a kind of collaboration that is difficult to imagine within existing retail incentives. Target’s approach shows how easily predictive power becomes a tool for subtle influence. When prediction is tied to increased profit, corporations loose all incentive to collaborate or empathize with the concerns of consumers."
  },
  {
    "objectID": "ethics_blog.html#why-this-matters-who-benefits-who-is-neglected-or-harmed",
    "href": "ethics_blog.html#why-this-matters-who-benefits-who-is-neglected-or-harmed",
    "title": "Project 4: Predictive Analytics at Target",
    "section": "",
    "text": "The Target case demonstrates how predictive analytics, when deployed without meaningful consent or accountability, can reinforce existing power imbalances. The company gained a competitive advantage through early detection of pregnancy and stronger customer retention. For consumers, however, the costs were harder to see but deeply felt: a loss of control over how their personal information and the inferences built from it circulated through their lives.\nHBR encourages us to view this as part of a broader question about who gets to shape the landscape of predictive technology and what the implications of there choices are. That framing is useful, but only if we recognize that transparency alone cannot resolve the underlying asymmetry. A technically sophisticated model can still fail sociall or ethicall standards, because people do not experience systems as abstractions, their experiences are formed by the ways those systems touch their everyday lives.\nLooking across other data-driven systems, the pattern is familiar. Technologies built to optimize efficiency often overlook the realities of those they are applied to. It’s an important reminder of the ways that code can touch peoples lives, and that efficiency (especially when geared towards profit) is far from the only metric to needed to create ethical processing systems. In the case of Target, we see how when predictive systems operate quietly, without clear boundaries, they risk crossing lines that individuals never had the chance to draw themselves."
  },
  {
    "objectID": "ethics_blog.html#citations",
    "href": "ethics_blog.html#citations",
    "title": "Project 4: Predictive Analytics at Target",
    "section": "",
    "text": "Duhigg, C. (2012). How Companies Learn Your Secrets. The New York Times Magazine, February 19.\nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\nJohn, L. K., Kim, T., & Barasz, K. (2020). Customer Data: Designing for Transparency and Trust. Harvard Business Review, May–June.\nhttps://hbr.org/2020/05/customer-data-designing-for-transparency-and-trust"
  },
  {
    "objectID": "Permutation_test.html",
    "href": "Permutation_test.html",
    "title": "Permutation Testing",
    "section": "",
    "text": "In this project, I conduct a small simulation study using a permutation test to explore whether there is an association between warehouse construction and unemployment rates in Southern California between 1990 and 2023.\nSpecifically, I ask:\n    Is there a relationship between economic growth through warehouse construction and local unemployment levels in Los Angeles, San Bernardino, and Riverside Counties from 1990 to 2023?\nThis question is interesting because warehouse development is often seen as an engine of local job creation in the Inland Empire, but data can reveal whether such development truly corresponds with lower unemployment."
  },
  {
    "objectID": "Permutation_test.html#introduction",
    "href": "Permutation_test.html#introduction",
    "title": "Permutation Testing",
    "section": "",
    "text": "In this project, I conduct a small simulation study using a permutation test to explore whether there is an association between warehouse construction and unemployment rates in Southern California between 1990 and 2023.\nSpecifically, I ask:\n    Is there a relationship between economic growth through warehouse construction and local unemployment levels in Los Angeles, San Bernardino, and Riverside Counties from 1990 to 2023?\nThis question is interesting because warehouse development is often seen as an engine of local job creation in the Inland Empire, but data can reveal whether such development truly corresponds with lower unemployment."
  },
  {
    "objectID": "Permutation_test.html#data-used",
    "href": "Permutation_test.html#data-used",
    "title": "Permutation Testing",
    "section": "Data Used:",
    "text": "Data Used:\nWarehouseCITY dataset1 — total new warehouse square footage built per year (1990–2023).\nLocal Area Unemployment Statistics (LAUS) from the U.S. Bureau of Labor Statistics2 — annual unemployment rate for the same region.\nBoth of the above data sets were sourced and trimmed down to only the fields relevant to this question during a an old project I had been working on.\n\n\nCode\nnew_sqft_by_unemployment &lt;- tribble(\n  ~Year, ~Building_sqft, ~Unemployment_rate,\n  1990, 33117000, 5.5,\n  1991, 18049000, 7.7,\n  1992, 13342000, 9.4,\n  1993, 5496000, 9.6,\n  1994, 9896000, 8.6,\n  1995, 12322000, 7.6,\n  1996, 13340000, 7.4,\n  1997, 18313000, 6.2,\n  1998, 40683000, 5.8,\n  1999, 37391000, 5.1,\n  2000, 43402000, 4.9,\n  2001, 43587000, 5.3,\n  2002, 29485000, 6.4,\n  2003, 32292000, 6.5,\n  2004, 36145000, 5.9,\n  2005, 37454000, 5.0,\n  2006, 36564000, 4.5,\n  2007, 36654000, 5.0,\n  2008, 22418000, 7.3,\n  2009, 12283000, 11.6,\n  2010, 4915000, 12.4,\n  2011, 8212000, 11.9,\n  2012, 13895000, 10.6,\n  2013, 18823000, 9.2,\n  2014, 23496000, 7.7,\n  2015, 29009000, 6.3,\n  2016, 28887000, 5.2,\n  2017, 37838000, 4.7,\n  2018, 45736000, 4.2,\n  2019, 30398000, 4.1,\n  2020, 37704000, 11.1,\n  2021, 43896000, 8.0,\n  2022, 44088000, 4.5,\n  2023, 30926000, 4.7\n)"
  },
  {
    "objectID": "Permutation_test.html#visualizing-the-original-relationship",
    "href": "Permutation_test.html#visualizing-the-original-relationship",
    "title": "Permutation Testing",
    "section": "Visualizing the Original Relationship",
    "text": "Visualizing the Original Relationship\nThe following scatterplot displays the annual relationship between new warehouse square footage and unemployment rate. Each point represents one year between 1990 and 2023, with a linear trend line to visualize direction.\n\n\nCode\nnew_sqft_by_unemployment |&gt;\n  ggplot(aes(x = Building_sqft, y = Unemployment_rate)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Building Square Footage vs. Unemployment Rate\",\n       x = \"New building sq.ft. by year\",\n       y = \"Unemployment rate (%)\") +\n  scale_x_continuous(labels = comma_format(accuracy = 1)) +\n  theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIn this graph we see years with higher construction volumes tend to have lower unemployment, suggesting a potential negative correlation (although not an extremely strong one). However, to test whether this observed relationship could have arisen by random chance, we perform a permutation test."
  },
  {
    "objectID": "Permutation_test.html#setting-up-the-permutation-test",
    "href": "Permutation_test.html#setting-up-the-permutation-test",
    "title": "Permutation Testing",
    "section": "Setting Up the Permutation Test",
    "text": "Setting Up the Permutation Test\nMy hypotheses are:\n  Null hypothesis (H₀): There is no association between warehouse construction and unemployment rate.\n  Alternative hypothesis (H₁): Higher levels of warehouse construction correspond with lower unemployment.\nThe test statistic will be the correlation coefficient between Building_sqft and Unemployment_rate."
  },
  {
    "objectID": "Permutation_test.html#simulation-design",
    "href": "Permutation_test.html#simulation-design",
    "title": "Permutation Testing",
    "section": "Simulation Design",
    "text": "Simulation Design\nI’ll first write a small function that computes a correlation after shuffling one variable. Then I’ll use a map() variant to repeat this process thousands of times, generating the null distribution.\n\n\nCode\n# Function to compute one permuted correlation\n# perm_cor for permutation correlation\nperm_cor &lt;- function(df) {\ncor(df$Building_sqft, sample(df$Unemployment_rate))\n}\n\n# Run simulations using map_dbl()\n\nset.seed(47)\nn_sims &lt;- 10000\nr_perm &lt;- map_dbl(1:n_sims, ~ perm_cor(new_sqft_by_unemployment))\n\n# Observed correlation in original data\nr_obs &lt;- cor(new_sqft_by_unemployment$Building_sqft,\nnew_sqft_by_unemployment$Unemployment_rate)\n\n# Two-sided p-value\np_val &lt;- mean(abs(r_perm) &gt;= abs(r_obs))\n\ntibble(observed_r = r_obs, p_value_two_sided = p_val)\n\n\n# A tibble: 1 × 2\n  observed_r p_value_two_sided\n       &lt;dbl&gt;             &lt;dbl&gt;\n1     -0.736                 0"
  },
  {
    "objectID": "Permutation_test.html#visualizing-the-permutation-distribution",
    "href": "Permutation_test.html#visualizing-the-permutation-distribution",
    "title": "Permutation Testing",
    "section": "Visualizing the Permutation Distribution",
    "text": "Visualizing the Permutation Distribution\nThe histogram below shows the distribution of correlation coefficients obtained under the null hypothesis (no relationship). The vertical red line marks the observed correlation from the real data.\n\n\nCode\ntibble(r = r_perm) |&gt;\nggplot(aes(x = r)) +\ngeom_histogram(bins = 40, fill = \"skyblue\", color = \"white\") +\ngeom_vline(xintercept = r_obs, color = \"red\", linewidth = 1.2) +\nlabs(\ntitle = \"Permutation Test for Correlation\",\nsubtitle = paste0(\"Observed r = \", round(r_obs, 3),\n\" | p(two-sided) = \", signif(p_val, 3)),\nx = \"Permuted correlation (null distribution)\",\ny = \"Count\"\n) +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nThe histogram is centered around 0, as expected under the null hypothesis. The observed correlation of approximately −0.74 lies far in the left tail of the distribution. Out of 10,000 permutations, none produced a correlation as extreme, implying a very small p-value and evidence against the null hypothesis."
  },
  {
    "objectID": "Permutation_test.html#discussion",
    "href": "Permutation_test.html#discussion",
    "title": "Permutation Testing",
    "section": "Discussion",
    "text": "Discussion\nThis simulation suggests a negative relationship between warehouse construction and unemployment in the Los Angeles–San Bernardino–Riverside region from 1990 to 2023, meaning that in years when more warehouse square footage was built, unemployment rates tended to be lower. However, it’s important to note that while the null hypothesis was disproved, that doesn’t mean the initial data shown at the top has a strong linear correlation (which is why this data got scrapped from the project i was originally using it for)\nFurthermore, this result does not imply direct causation. Both variables may be influenced by larger economic cycles (e.g., recessions in 2008 and 2020). Still, this permutation study demonstrated a clear relationship in the data, that with the help of permutation testing we can now attribute to more than just chance."
  },
  {
    "objectID": "Permutation_test.html#footnotes",
    "href": "Permutation_test.html#footnotes",
    "title": "Permutation Testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPhillips, S. A., & McCarthy, M. C. (2024). Warehouse CITY: An open data product for evaluating warehouse land-use in Southern California. Environment and Planning B, 51(8), 1965–1973. https://doi.org/10.1177/23998083241262553↩︎\nU.S. Bureau of Labor Statistics. (2024). Local Area Unemployment Statistics (LAUS). https://www.bls.gov/lau/↩︎"
  },
  {
    "objectID": "tidytuesday-2.html",
    "href": "tidytuesday-2.html",
    "title": "TidyTuesday - Gas Pricing",
    "section": "",
    "text": "The following data was provided by TidyTuesday, linked here:\nhttps://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-07-01/readme.md\nThe data was sourced from the U.S. Energy Information Administration (EIA), which publishes average retail gasoline and diesel prices each Monday. The original data (including additional datasets) can be found at eia.gov/petroleum/gasdiesel\nThe following code averages out weekly reports to create average annual gasoline prices. Then, the data is graphed to create a visual display of the change in American gas pricing over time.\n\n# yearly average gasoline price\ngas_yearly &lt;- weekly_gas_prices |&gt;\n  filter(fuel == \"gasoline\", grade == \"all\", formulation == \"all\") |&gt;\n  mutate(year = year(date)) |&gt;\n  group_by(year) |&gt;\n  summarise(avg_price = mean(price, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(year)\n\nprint(gas_yearly, n = Inf)\n\n# A tibble: 33 × 2\n    year avg_price\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1  1993      1.07\n 2  1994      1.08\n 3  1995      1.16\n 4  1996      1.24\n 5  1997      1.24\n 6  1998      1.07\n 7  1999      1.18\n 8  2000      1.52\n 9  2001      1.46\n10  2002      1.39\n11  2003      1.60\n12  2004      1.89\n13  2005      2.31\n14  2006      2.62\n15  2007      2.84\n16  2008      3.30\n17  2009      2.41\n18  2010      2.84\n19  2011      3.58\n20  2012      3.68\n21  2013      3.58\n22  2014      3.44\n23  2015      2.52\n24  2016      2.25\n25  2017      2.53\n26  2018      2.81\n27  2019      2.69\n28  2020      2.26\n29  2021      3.10\n30  2022      4.06\n31  2023      3.63\n32  2024      3.42\n33  2025      3.25\n\n\n\n# Plot\nggplot(gas_yearly, aes(x = year, y = avg_price, group = 1)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Average U.S. Gasoline Price by Year\",\n    x = \"Year\",\n    y = \"Average price (USD/gal)\"\n  ) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  theme_minimal(base_size = 12)"
  },
  {
    "objectID": "tidytuesday-1.html",
    "href": "tidytuesday-1.html",
    "title": "TidyTuesday - Himalayan Mountaineering",
    "section": "",
    "text": "The following data set was gathered and provided by TidyTuesday, linked here:\nhttps://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-01-21/readme.md\nThe data was sourced from the Himalayan Database which keeps information on peaks and mountaineering expeditions that take place in the Himalayan Region. The full himalayan database catalog can be found here:\nhttps://www.himalayandatabase.com/index.html\nThe following graph attempts to compare the volume of expeditions per region listed in the data on all mountains exceeding 7,000 meters. This is done by joining the “peaks” and “expeditions” data sets, filtering for only peaks above 7,000m, and then counting the expeditions by region.\n\n# joining\nexped_tidy &lt;- exped_tidy |&gt; mutate(PEAKID = as.character(PEAKID))\npeaks_tidy &lt;- peaks_tidy |&gt; mutate(PEAKID = as.character(PEAKID))\n\n# count expeditions for peaks &gt;= 7000m \nexped_by_region &lt;- exped_tidy |&gt;\n  distinct(EXPID, PEAKID) |&gt;  \n  inner_join(\n    peaks_tidy |&gt; select(PEAKID, HEIGHTM, REGION_FACTOR),\n    by = \"PEAKID\"\n  ) |&gt;\n  filter(HEIGHTM &gt;= 7000) |&gt;\n  count(REGION_FACTOR, name = \"n_expeditions\") |&gt;\n  arrange(desc(n_expeditions))\n\nexped_by_region\n\n# A tibble: 6 × 2\n  REGION_FACTOR           n_expeditions\n  &lt;chr&gt;                           &lt;int&gt;\n1 Khumbu-Rolwaling-Makalu           356\n2 Manaslu-Ganesh                     96\n3 Annapurna-Damodar-Peri             90\n4 Dhaulagiri-Mukut                   45\n5 Kangchenjunga-Janak                28\n6 Langtang-Jugal                      2\n\n\n\n# ---- plot ----\nggplot(exped_by_region,\n       aes(x = forcats::fct_reorder(REGION_FACTOR, n_expeditions),\n           y = n_expeditions)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Number of Expeditions on ≥7000 m Peaks, by Region\",\n    subtitle = \"Himalayan Database (TidyTuesday 2025-01-21)\",\n    x = \"Region\",\n    y = \"Number of expeditions\"\n  ) +\n  theme_minimal(base_size = 12)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! My name is Henry Nadel. I am currently in my final year at Pitzer College where I study Environmental Analysis on a public policy track with an additional minor in Data Science. I am particularly interested in using data to inform public policy along with using graphing and GIS software to convey data in an accessible manner. When I’m not in the classroom I can usually be found outside running, climbing, or exploring one of California’s amazing parks.\nCurrently: Senior at Pitzer College\nMajor: Environmental Analysis; Minor: Data Science\nBased in: Claremont, CA\nOriginally from: Newton, MA"
  },
  {
    "objectID": "text_analysis.html",
    "href": "text_analysis.html",
    "title": "Analysing New York Times Headlines",
    "section": "",
    "text": "On this page I analyze a data set with headlines from all New York Times articles dating back to 1996. My analysis focuses on (1) frequent content words, (2) locations referenced, and (3) how often numbers and years appear in headlines.\nThis data was soruced from the NYTimes dataset in the RTextTools package."
  },
  {
    "objectID": "text_analysis.html#frequest-word-usage",
    "href": "text_analysis.html#frequest-word-usage",
    "title": "Analysing New York Times Headlines",
    "section": "Frequest Word Usage",
    "text": "Frequest Word Usage\n\ntitles_tokens &lt;- nyt |&gt;\n  transmute(\n    Title_clean = Title |&gt;\n      str_to_lower() |&gt;\n      str_replace_all(\"[^a-z\\\\s]\", \" \") |&gt;\n      str_replace_all(\"\\\\s+\", \" \") |&gt;\n      str_trim()\n  ) |&gt;\n  unnest_tokens(word, Title_clean, token = \"words\") |&gt;\n  anti_join(stop_words, by = \"word\") |&gt;\n  filter(str_detect(word, \"^[a-z]+$\"), nchar(word) &gt;= 3)\n\ntop_words &lt;- titles_tokens |&gt;\n  count(word, sort = TRUE) |&gt;\n  slice_head(n = 15)\n\n\n# Plot — Top content words\nggplot(top_words, aes(x = reorder(word, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Most Frequent\n    VWords in NYT Headlines since 1996\",\n    x = \"Word\",\n    y = \"Count\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nThis bar chart shows the most common words in found in the headline column of the data set. It’s an interesting way to find many of the most common topics covered by the New York Times over the past three decades. Notably, the top three words stand distinctively above the rest in usage: Bush, Iraq, and War. this makes sense as all three of these words point to the same general event of President Bush’s decision to invade Iraq following 9/11. This makes sense as it was a large global event that toom place near the beginning of this data’s period of observation. These findings make an interesting claim about how extensively the war in Iraq penetrated the American news cycle."
  },
  {
    "objectID": "text_analysis.html#frequently-mentioned-places",
    "href": "text_analysis.html#frequently-mentioned-places",
    "title": "Analysing New York Times Headlines",
    "section": "Frequently Mentioned Places",
    "text": "Frequently Mentioned Places\nin this section I find the locations that are most frequestly refrenced by searcing for words that follow the preposition “in…”\n\nlocs &lt;- nyt |&gt;\n  transmute(\n    Title,\n    loc = str_extract_all(\n      Title,\n      \"(?&lt;=\\\\bin\\\\s)([A-Z][A-Za-z\\\\-]+(?:\\\\s[A-Z][A-Za-z\\\\-]+)*)\"\n    )\n  ) |&gt;\n  unnest(loc) |&gt;\n  mutate(loc = loc |&gt;\n           str_replace_all(\"\\\\s+\", \" \") |&gt;\n           str_trim()) |&gt;\n  count(loc, sort = TRUE) |&gt;\n  slice_head(n = 10)\n\n\n# Plot 2 — Top phrases following \"in \"\nggplot(locs, aes(x = reorder(loc, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Phrases Following 'in' - NYT Headlines\",\n    x = \"Location phrase\",\n    y = \"Count\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nFollowing the previous section, it is unsurprising that far and away the most frequently mentioned location is Iraq, further emphasizing the impact of the war in Iraq on the american news cycle. Another interesting finding from this is a lack of reference to Africa or South America in the top 10 spots. Furthermore, New York and Florida stick out as the only U.S. states listed. It is worth noting that many of these names have much higher tallies when counting total uses. The specific criteria of words following “in” is meant to isolate the places where stories are taking place, opposed to total mentions."
  },
  {
    "objectID": "text_analysis.html#headlines-containing-numbers-years",
    "href": "text_analysis.html#headlines-containing-numbers-years",
    "title": "Analysing New York Times Headlines",
    "section": "Headlines Containing Numbers & Years",
    "text": "Headlines Containing Numbers & Years\n\ntitle_nums &lt;- nyt |&gt;\n  transmute(\n    Title,\n    has_number = str_detect(Title, \"\\\\b\\\\d{1,3}(?:,\\\\d{3})*(?:\\\\.\\\\d+)?\\\\b\"),\n    years_list = str_extract_all(Title, \"\\\\b(19|20)\\\\d{2}\\\\b\")\n  )\n\nprop_table &lt;- tibble(\n  metric = c(\"Headlines containing any number\"),\n  value  = c(mean(title_nums$has_number))\n) |&gt;\n  mutate(share = scales::percent(value)) |&gt;\n  select(metric, share)\n\n\n# Table — Share of headlines with numbers / percentages\nkable(\n  prop_table,\n  col.names = c(\"Metric\", \"Share of headlines\"),\n  caption = \"% of NYT headlines with numeric content\"\n)\n\n\n% of NYT headlines with numeric content\n\n\nMetric\nShare of headlines\n\n\n\n\nHeadlines containing any number\n11%\n\n\n\n\n\n\n# Table — Most mentioned years\nyears_top &lt;- title_nums |&gt;\n  select(years_list) |&gt;\n  unnest(years_list, keep_empty = FALSE) |&gt;\n  count(year = years_list, sort = TRUE) |&gt;\n  slice_head(n = 10)\n\nkable(\n  years_top,\n  col.names = c(\"Year\", \"Count\"),\n  caption = \"Top 10 years referenced in headlines\"\n)\n\n\nTop 10 years referenced in headlines\n\n\nYear\nCount\n\n\n\n\n2000\n37\n\n\n1998\n6\n\n\n2004\n3\n\n\n1964\n2\n\n\n1996\n2\n\n\n2003\n2\n\n\n1937\n1\n\n\n1940\n1\n\n\n1970\n1\n\n\n1972\n1\n\n\n\n\n\nIt’s interesting how much of this list is years before the period of observation from the data set beginning in 1996. This indicates a trend of using years in headlines more frequently when referring to events farther back in the past. It’s also worth noting the “Count” column, which shows that they year 2000 is mentioned by far the most frequently, and all but six years were mentioned only once.\nData Citation: R Core Team. (2020). NYTimes: a sample dataset containing labeled headlines from The New York Times. RTextTools. https://rdrr.io/cran/RTextTools/man/NYTimes.html"
  },
  {
    "objectID": "sql_project.html",
    "href": "sql_project.html",
    "title": "sql_project",
    "section": "",
    "text": "Code\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\n\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname   = \"traffic\",\n  host     = Sys.getenv(\"TRAFFIC_HOST\"),\n  user     = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)"
  },
  {
    "objectID": "sql_project.html#overview",
    "href": "sql_project.html#overview",
    "title": "sql_project",
    "section": "Overview",
    "text": "Overview\nIn this project, I look at policing patterns across four New England states: Connecticut, Massachusetts, Rhode Island, and Vermont, using data from the Stanford Open Policing Project. Unfortunatly, New Hampture could not be included because the data did not include all of the necessary values. The goal of this research is simple: use SQL to pull together large statewide stop records, and then use R to visualize my two core questions:\n\nWhat is the racial breakdown of who is being searched during traffic stops?\nWhen searches do happen, at what rates is contraband found?\n\nThese questions were chosen because of the light they can shed on where officer discretion, racial bias, and assumptions about criminality become visible. By comparing search rates and hit rates side-by-side, we can see where enforcement patterns align with actual evidence, and where they don’t."
  },
  {
    "objectID": "sql_project.html#search-rates-by-state-and-race",
    "href": "sql_project.html#search-rates-by-state-and-race",
    "title": "sql_project",
    "section": "Search Rates by State and Race",
    "text": "Search Rates by State and Race\n\nSELECT\nstate,\nrace,\nCOUNT(*) AS n_stops,\nSUM(search_conducted) AS n_searches,\nROUND(100.0 * SUM(search_conducted) / COUNT(*), 1) AS search_rate_pct\nFROM (\nSELECT\n'CT' AS state,\nLOWER(subject_race) AS race,\nsearch_conducted,\ncontraband_found\nFROM ct_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n\nUNION ALL\nSELECT\n'MA',\nLOWER(subject_race),\nsearch_conducted,\ncontraband_found\nFROM ma_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n\nUNION ALL\nSELECT\n'RI',\nLOWER(subject_race),\nsearch_conducted,\ncontraband_found\nFROM ri_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n\nUNION ALL\nSELECT\n'VT',\nLOWER(subject_race),\nsearch_conducted,\ncontraband_found\nFROM vt_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n) AS ne_statewide\nGROUP BY state, race\nHAVING COUNT(*) &gt;= 100\nORDER BY state, race\n\n\n\nCode\nsearch_rates$race &lt;- factor(search_rates$race)\nsearch_rates$state &lt;- factor(search_rates$state)\n\nggplot(search_rates,\naes(x = state, y = search_rate_pct, fill = race)) +\ngeom_col(position = \"dodge\") +\nscale_fill_brewer(palette = \"Set2\") +\nlabs(\ntitle = \"Search Rates by State and Race (CT, MA, RI, VT)\",\nx = \"State\",\ny = \"Search Rate (% of Stops)\",\nfill = \"Race\"\n) +\ntheme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nThis plot shows that search rates are not evenly distributed across racial groups, and the patterns differ across states. Across every state shown above, Black and Hispanic drivers are searched at substantially higher rates than white drivers. This is especially pronounced in Connecticut and Rhode Island, where Black drivers face search rates around 7%, compared to roughly 3% for white drivers. Hispanic drivers show similarly elevated rates—around 6–7% in CT and RI.\nMassachusetts and Vermont have lower overall search activity, but the racial pattern still remains: Black and Hispanic drivers are searched more than white drivers, while Asian/Pacific Islander drivers consistently face the lowest search rates across the region. The “other” and “unknown” categories vary but rarely approach the higher search pressure placed on Black and Hispanic motorists. What emerges here is not just cross-state variation but a consistent structure: race strongly predicts the likelihood of being searched, and white drivers are the least likely to be searched in every state in the dataset."
  },
  {
    "objectID": "sql_project.html#contraband-hit-rates-by-state-and-race-among-searched-stops",
    "href": "sql_project.html#contraband-hit-rates-by-state-and-race-among-searched-stops",
    "title": "sql_project",
    "section": "Contraband Hit Rates by State and Race (Among Searched Stops)",
    "text": "Contraband Hit Rates by State and Race (Among Searched Stops)\n\n\nSELECT\nstate,\nrace,\nSUM(search_conducted) AS n_searches,\nSUM(CASE WHEN contraband_found = 1 THEN 1 ELSE 0 END) AS n_hits,\nROUND(\n100.0 *\nSUM(CASE WHEN contraband_found = 1 THEN 1 ELSE 0 END)\n/ NULLIF(SUM(search_conducted), 0),\n1\n) AS hit_rate_pct\nFROM (\nSELECT\n'CT' AS state,\nLOWER(subject_race) AS race,\nsearch_conducted,\ncontraband_found\nFROM ct_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n\nUNION ALL\nSELECT\n'MA',\nLOWER(subject_race),\nsearch_conducted,\ncontraband_found\nFROM ma_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n\nUNION ALL\nSELECT\n'RI',\nLOWER(subject_race),\nsearch_conducted,\ncontraband_found\nFROM ri_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n\nUNION ALL\nSELECT\n'VT',\nLOWER(subject_race),\nsearch_conducted,\ncontraband_found\nFROM vt_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\n) AS ne_statewide\nGROUP BY state, race\nHAVING SUM(search_conducted) &gt;= 50\nORDER BY state, race\n\n\n\nCode\nhit_rates$race &lt;- factor(hit_rates$race)\n\nggplot(hit_rates,\naes(x = race, y = hit_rate_pct, fill = state)) +\ngeom_col(position = \"dodge\") +\nscale_fill_brewer(palette = \"Set2\") +\nlabs(\ntitle = \"Contraband Hit Rates by Race and State (Among Searched Stops)\",\nx = \"Race\",\ny = \"Hit Rate (% of Searches)\"\n) +\ntheme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nThe second plot flips the lens: instead of who gets searched, it explores what police actually find when they conduct a search. The results tell a different story, one that goes against an assumption that higher search rates correspond to higher levels of contraband.\nAcross states, white drivers consistently show some of the highest contraband hit rates, especially in Vermont, where searches of white drivers turn up contraband nearly 90% of the time. Black and Hispanic drivers, who are searched far more often, show much lower hit rates—usually ranging from the high teens to the mid-40s depending on the state. This mismatch—higher search rates paired with lower hit rates—is especially stark in Connecticut and Rhode Island.\nIn short, the communities searched most aggressively are not the ones most likely to be found with contraband. Instead, white drivers, searched the least often, show the highest probability of a “successful” search.\nThis pattern displays a clear racial disparity in search rates, and shows that they are driven more by bias and over-policing, rather than by actual behavior."
  },
  {
    "objectID": "sql_project.html#conclusion",
    "href": "sql_project.html#conclusion",
    "title": "sql_project",
    "section": "Conclusion",
    "text": "Conclusion\nBy merging four statewide datasets and visualizing search and hit rates, a clear pattern emerges: Black and Hispanic drivers are searched far more often, yet those searches uncover contraband less frequently than searches of white drivers. This holds across every state in the dataset. In other words, the groups searched the most are not the ones most likely to have contraband. That mismatch points toward discretionary enforcement shaped more by racial bias than by evidence. Even with New Hampshire excluded, the trend is unmistakable: search practices in these New England states fall unevenly along racial lines, and the data make that disparity impossible to explain away on the basis of “risk.\n\nDBI::dbDisconnect(con_traffic)"
  }
]